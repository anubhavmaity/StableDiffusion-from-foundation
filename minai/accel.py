# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/lectures/17_DDPM_v2.ipynb.

# %% auto 0
__all__ = ['MixedPrecision', 'AccelerateCB']

# %% ../nbs/lectures/17_DDPM_v2.ipynb 44
class MixedPrecision(TrainCB):
    order = DeviceCB.order + 10
    
    def before_fit(self, learn): self.scaler = torch.cuda.amp.GradScaler()
    
    def before_batch(self, learn):
        self.autocast = torch.autocast("cuda", dtype=torch.float16)
        self.autocast.__enter__()
    
    def after_loss(self, learn):
        self.autocast.__exit__(None, None, None)
    
    def backward(self, learn):
        self.scaler.scale(learn.loss).backward()
    
    def step(self, learn):
        self.scaler.step(learn.opt)
        self.scaler.update()

# %% ../nbs/lectures/17_DDPM_v2.ipynb 51
class AccelerateCB(TrainCB):
    order = DeviceCB.order + 10
    
    def __init__(self, n_inp=1, mixed_precision='fp16'):
        super().__init__(n_inp=n_inp)
        self.acc = Accelerator(mixed_precision=mixed_precision)
    
    def before_fit(self, learn):
        learn.model, learn.opt, learn.dls.train, learn.dls.valid = self.acc.prepare(learn.model, learn.opt, learn.dls.train, learn.dls.valid)
    
    def backward(self, learn):  self.acc.backward(learn.loss)
