{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b450cd4",
   "metadata": {},
   "source": [
    "# Minibatch Training KMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71398c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, gzip, math, os, time, shutil, torch, matplotlib as mpl, numpy as np\n",
    "import numpy as np\n",
    "from torch import tensor\n",
    "from pathlib import Path\n",
    "from fastcore.test import test_close\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "mpl.rcParams['image.cmap'] = 'gray'\n",
    "torch.set_printoptions(precision=2, linewidth=125, sci_mode=False)\n",
    "np.set_printoptions(precision=2, linewidth=125)\n",
    "\n",
    "path_data = Path('data')\n",
    "\n",
    "train_images_file = 'kmnist-train-imgs.npz'\n",
    "train_labels_file = 'kmnist-train-labels.npz'\n",
    "test_images_file = 'kmnist-test-imgs.npz'\n",
    "test_labels_file = 'kmnist-test-labels.npz'\n",
    "\n",
    "x_train = np.load(path_data/train_images_file)['arr_0'].reshape(-1, 784)/255.0\n",
    "x_train = np.float32(x_train)\n",
    "y_train = np.load(path_data/train_labels_file)['arr_0']\n",
    "x_valid = np.load(path_data/test_images_file)['arr_0'].reshape(-1, 784)/255.0\n",
    "x_valid = np.float32(x_valid)\n",
    "y_valid = np.load(path_data/test_labels_file)['arr_0']\n",
    "\n",
    "x_train, y_train, x_valid, y_valid = map(tensor, (x_train, y_train, x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610a4739",
   "metadata": {},
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7589ea47",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fd0463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784, tensor(10, dtype=torch.uint8))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n,m = x_train.shape\n",
    "c = y_train.max() + 1\n",
    "nh = 50\n",
    "n, m, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161cea38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        super().__init__()\n",
    "        self.layers = [nn.Linear(n_in, nh), nn.ReLU(), nn.Linear(nh, n_out)]\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd5a85c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 10])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(m, nh, 10)\n",
    "pred = model(x_train)\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eded7476",
   "metadata": {},
   "source": [
    "### Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e27b2e0",
   "metadata": {},
   "source": [
    "First, we need to compute the softmax of our activations. This defined by: \n",
    "    \n",
    "$$ \\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{e^{x_{0}} + e^{x_{1}} + \\cdots + e^{x_{n-1}}}$$\n",
    "\n",
    "or more concisely:\n",
    "\n",
    "$$ \\hbox{softmax(x)}_{i} =  \\frac{e^{x_{i}}}{\\sum\\limits_{0 \\leq j \\lt n} e^{x_{j}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5c3a67",
   "metadata": {},
   "source": [
    "In practice, we will need the log of softmax when we calculate the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b22e2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x): return (x.exp() / (x.exp().sum(-1, keepdim=True))).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6761419d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.35, -2.37, -2.08,  ..., -2.47, -2.45, -2.20],\n",
       "        [-2.38, -2.34, -2.07,  ..., -2.50, -2.55, -2.17],\n",
       "        [-2.36, -2.48, -1.95,  ..., -2.47, -2.46, -2.13],\n",
       "        ...,\n",
       "        [-2.39, -2.39, -2.04,  ..., -2.42, -2.46, -2.40],\n",
       "        [-2.45, -2.24, -2.22,  ..., -2.42, -2.46, -2.11],\n",
       "        [-2.30, -2.22, -2.09,  ..., -2.43, -2.44, -2.29]], grad_fn=<LogBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_softmax(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd73002b",
   "metadata": {},
   "source": [
    "Note that the formula\n",
    "\n",
    "$$ \\log \\left ( \\frac{a}{b} \\right ) = \\log(a) - \\log(b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79362a36",
   "metadata": {},
   "source": [
    "gives a simplification when we compute the log softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1333f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x): return x - x.exp().sum(-1, keepdim=True).log()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78455c76",
   "metadata": {},
   "source": [
    "Then, there is a way to compute the log of the sum of exponentials in a more stable way, called the LogSumExp trick. The idea is to use the following formula\n",
    "\n",
    "$$\\log \\left ( \\sum_{j=1}^{n} e^{x_{j}} \\right ) = \\log \\left ( e^{a} \\sum_{j=1}^{n} e^{x_{j}-a} \\right ) = a + \\log \\left ( \\sum_{j=1}^{n} e^{x_{j}-a} \\right )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57844862",
   "metadata": {},
   "source": [
    "where a is the max of x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74a7a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsumexp(x):\n",
    "    m = x.max(-1)[0]\n",
    "    return m + (x  - m[:, None]).exp().sum(-1).log()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd1c535",
   "metadata": {},
   "source": [
    "This way, we will avoid an overflow when taking the exponential of a big activation. In PyTorch, this is already implemented for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699b60cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x): return x - x.logsumexp(-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadc29c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.35, -2.37, -2.08,  ..., -2.47, -2.45, -2.20],\n",
       "        [-2.38, -2.34, -2.07,  ..., -2.50, -2.55, -2.17],\n",
       "        [-2.36, -2.48, -1.95,  ..., -2.47, -2.46, -2.13],\n",
       "        ...,\n",
       "        [-2.39, -2.39, -2.04,  ..., -2.42, -2.46, -2.40],\n",
       "        [-2.45, -2.24, -2.22,  ..., -2.42, -2.46, -2.11],\n",
       "        [-2.30, -2.22, -2.09,  ..., -2.43, -2.44, -2.29]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_close(logsumexp(pred), pred.logsumexp(-1))\n",
    "sm_pred = log_softmax(pred)\n",
    "sm_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9d81ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8, 7, 0], dtype=torch.uint8)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63f71d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-2.45, grad_fn=<SelectBackward0>),\n",
       " tensor(-2.50, grad_fn=<SelectBackward0>),\n",
       " tensor(-2.36, grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_pred[0, 8], sm_pred[1, 7], sm_pred[2, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fbf183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 10])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a88fb32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 7, 0]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:3].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2d6c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.45, -2.50, -2.36], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_pred[[0, 1, 2],y_train[:3].long() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71028573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(input, target): return -input[range(target.shape[0]), target].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b002240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.31, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = nll(sm_pred, y_train.long())\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab42b37",
   "metadata": {},
   "source": [
    "Then use PyTorch's implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c920b4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(F.nll_loss(F.log_softmax(pred, -1), y_train.long()), loss, 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fea26b",
   "metadata": {},
   "source": [
    "In PyTorch, `F.logsoftmax` and `F.nll_loss` are combined in one optimized function `F.cross_entropy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c21827",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(F.cross_entropy(pred, y_train), loss, 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cecd30",
   "metadata": {},
   "source": [
    "### Basic training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e63e48d",
   "metadata": {},
   "source": [
    "Basically the training loop repeats over the following steps:\n",
    "- get the output of the model on batch of inputs\n",
    "- compare the output to the lables we have and compute loss\n",
    "- calculate the gradients of the loss with respect to every parameter of the model\n",
    "- update said parameters with those gradients to make them a little bit better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70811c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69aaa6ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.07, -0.09,  0.20, -0.14,  0.10, -0.09,  0.07, -0.19, -0.17,  0.08], grad_fn=<SelectBackward0>),\n",
       " torch.Size([64, 10]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 64\n",
    "\n",
    "xb = x_train[0:bs]\n",
    "preds = model(xb)\n",
    "preds[0], preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ebe876",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.33, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb = y_train[0:bs]\n",
    "loss_func(preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4038b22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 9, 2, 4, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 6, 2, 2, 2, 2, 4, 2, 2, 2,\n",
       "        2, 9, 2, 2, 9, 1, 2, 2, 1, 2, 6, 3, 9, 1, 2, 2, 9, 2, 1, 2, 2, 3, 4, 2, 2])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(preds, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0be916",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, yb): return (torch.argmax(out, dim=1) == yb).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ae350c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.08)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bb0cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.5 # learning rate\n",
    "epochs = 3 # how many epochs to train for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ee3353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.154031991958618 0.28125\n",
      "0.4318438172340393 0.890625\n",
      "0.3388446569442749 0.921875\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i in range(0, n, bs):\n",
    "        s = slice(i, min(n, i+bs))\n",
    "        xb, yb = x_train[s], y_train[s]\n",
    "        preds = model(xb)\n",
    "        loss = loss_func(preds, yb)\n",
    "        loss.backward()\n",
    "        if i==0: print(loss.item(), accuracy(preds, yb).item())\n",
    "        with torch.no_grad():\n",
    "            for l in model.layers:\n",
    "                if hasattr(l, 'weight'):\n",
    "                    l.weight -= l.weight.grad * lr\n",
    "                    l.bias -= l.bias.grad * lr\n",
    "                    l.weight.grad.zero_()\n",
    "                    l.bias.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06c22e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "fastai"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
